{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud ETL Pipeline for cryptocurrency market snapshot\n",
    "\n",
    "This project builds a cloud-native pipeline that extracts real-time cryptocurrency market data from the [CoinGecko API](https://www.coingecko.com/), stores data in **AWS S3 Bucket**, transforms it with **pandas**, and loads it into a **Snowflake** table for visualization in **Google Looker Studio**. All scripts are deployed in **AWS Lambda**, and orchestrated by **AWS Step Functions state machines**.\n",
    "\n",
    "## Pipeline Workflow\n",
    "\n",
    "![ETL tech diagram](etl_tech_diagram.png)\n",
    "\n",
    "- **Extract**:\n",
    "   - Data is fetched from the CoinGecko API using a Lambda function (`extract_crypto_data`).\n",
    "   - Raw JSON data is stored in an S3 bucket (`cm--raw-data`).\n",
    "\n",
    "- **Transform**:\n",
    "   - Data transformation is handled by another Lambda function (`transform_crypto_data`) using pandas to:\n",
    "     - Clean data\n",
    "     - Calculate additional fields (e.g., dominance percentage, volume percentage).\n",
    "   - Transformed data is stored as a CSV file in the same S3 bucket.\n",
    "\n",
    "- **Load**:\n",
    "   - A third Lambda function (`load_crypto_data`) loads the processed CSV data into Snowflake:\n",
    "     - Database: `CRYPTO_ETL_DB`\n",
    "     - Table: `CRYPTO_DATA`\n",
    "\n",
    "- **Orchestrate**:\n",
    "   - All lambda functions are orchestrated by a state machine (`crypto_etl_state_machine_light`)\n",
    "   - Output of previous state is used as input of next state to ensure seamless processing.\n",
    "   - Can be manually triggered on-demand\n",
    "\n",
    "- **Visualize**:\n",
    "   - Data in Snowflake table is connected to and visualized in Looker Studio\n",
    "     - [Crypto Market Snapshot](https://lookerstudio.google.com/s/gLuxBDnvJQg)\n",
    "     - Refresh data on-demand\n",
    "     - Provide a real-time snapshot of the global cryptocurrency market\n",
    "     - Metrics include market cap, price changes percentage etc.\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "- When cost is no longer a concern:\n",
    "   - Automate pipeline execution to enable trend analytics of snapshot data.\n",
    "   - Introduce **PySpark** with **AWS Glue** for big data.\n",
    "\n",
    "- Testing and quality assurance:\n",
    "   - Add alerting and monitoring for failures using **Amazon CloudWatch**.\n",
    "   - Add data quality checks in Snowflake.\n",
    "\n",
    "## Demo Run\n",
    "- The scripts in the repository are not runnable as they are for Lambda deployment.\n",
    "- To run the pipeline, use the credential file in the repo to log into AWS console as an **IAM user**.\n",
    "- Once logged in, search for \"state machines\" and click the corresponding service.\n",
    "- You should see a state machine called `crypto_etl_state_machine_light`.\n",
    "- Click it then choose `Start execution`, name your execution differently to history records then proceed.\n",
    "- For security concern minimum permissions are granted, you can't access Lambda functions involved or logs of the execution, but other details of the execution are avaialble.\n",
    "- After execution, open Looker report and click the blue three-dot on top right, select `Refresh data`, then click `FETCH_DATA`, you can now visualize the latest market data fetched by your run.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
